# Multimodal Classification of Dementia: Audio, Text and Timestamps
Dementia is a progressive neurological disorder that profoundly affects the daily lives of older adults, impairing abilities from verbal communication to cognitive function. Early diagnosis is essential for enhancing both lifespan and quality of life for affected individuals. Despite its importance, diagnosing dementia is complex and often necessitates a multi-modal approach incorporating diverse clinical data types. In this study, we apply transfer learning techniques to Wav2vec and Word2vec algorithms and train models using three distinct data types: audio recordings, text transcripts, and timestamps. We experiment with four conditions: original datasets versus datasets purged of short sentences, each with and without data augmentation. Our results indicate that data augmentation generally enhances model performance, underscoring the importance of data volume for achieving high accuracy. Additionally, models trained on text data frequently excel and can further improve the performance of other modalities when combined. In conclusion, the selection and integration of data modalities are crucial factors influencing the effectiveness of dementia diagnostic models.

##Dataset
The training and validation data utilized in this project were sourced from the English/Pitt dataset within the Dementia Databank (https://dementia.talkbank.org). This dataset contains timestamps of words and are useful in our project.

##Pre-trained models
Wav2vec (Baevski et al., 2020) is a self-supervised convolutional architecture that transforms audio waveforms into representative embeddings. Initially trained on unlabeled audio data, these embeddings are then processed through a transformer for a masked task. In this task, half of the audio embeddings are masked and predicted by the remaining unmasked portions. Wav2vec is particularly useful in speech recognition tasks due to its adaptability to various audio recordings and its  inherently robust performance.

Word2vec (Mikolov et al., 2013) is a feed-forward neural network designed to produce vector representations of words. It uses surrounding words as input to generate these vectors, capturing semantic relationships between the words. The resulting vectors effectively position semantically similar words closer in the vector space.
